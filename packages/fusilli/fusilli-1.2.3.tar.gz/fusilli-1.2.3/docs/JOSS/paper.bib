
@misc{klineMultimodalMachineLearning2022,
	title = {Multimodal {Machine} {Learning} in {Precision} {Health}},
	url = {http://arxiv.org/abs/2204.04777},
	abstract = {As machine learning and artificial intelligence are more frequently being leveraged to tackle problems in the health sector, there has been increased interest in utilizing them in clinical decision-support. This has historically been the case in single modal data such as electronic health record data. Attempts to improve prediction and resemble the multimodal nature of clinical expert decision-making this has been met in the computational field of machine learning by a fusion of disparate data. This review was conducted to summarize this field and identify topics ripe for future research. We conducted this review in accordance with the PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) extension for Scoping Reviews to characterize multi-modal data fusion in health. We used a combination of content analysis and literature searches to establish search strings and databases of PubMed, Google Scholar, and IEEEXplore from 2011 to 2021. A final set of 125 articles were included in the analysis. The most common health areas utilizing multi-modal methods were neurology and oncology. However, there exist a wide breadth of current applications. The most common form of information fusion was early fusion. Notably, there was an improvement in predictive performance performing heterogeneous data fusion. Lacking from the papers were clear clinical deployment strategies and pursuit of FDA-approved tools. These findings provide a map of the current literature on multimodal data fusion as applied to health diagnosis/prognosis problems. Multi-modal machine learning, while more robust in its estimations over unimodal methods, has drawbacks in its scalability and the time-consuming nature of information concatenation.},
	urldate = {2022-10-06},
	publisher = {arXiv},
	author = {Kline, Adrienne and Wang, Hanyin and Li, Yikuan and Dennis, Saya and Hutch, Meghan and Xu, Zhenxing and Wang, Fei and Cheng, Feixiong and Luo, Yuan},
	month = apr,
	year = {2022},
	note = {arXiv:2204.04777 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/florencetownend/Zotero/storage/S48IA76T/Kline et al. - 2022 - Multimodal Machine Learning in Precision Health.pdf:application/pdf;arXiv.org Snapshot:/Users/florencetownend/Zotero/storage/UDBLY5BN/2204.html:text/html},
}

@inproceedings{antelmiSparseMultiChannelVariational2019,
	title = {Sparse {Multi}-{Channel} {Variational} {Autoencoder} for the {Joint} {Analysis} of {Heterogeneous} {Data}},
	url = {https://proceedings.mlr.press/v97/antelmi19a.html},
	abstract = {Interpretable modeling of heterogeneous data channels is essential in medical applications, for example when jointly analyzing clinical scores and medical images. Variational Autoencoders (VAE) are powerful generative models that learn representations of complex data. The flexibility of VAE may come at the expense of lack of interpretability in describing the joint relationship between heterogeneous data. To tackle this problem, in this work we extend the variational framework of VAE to bring parsimony and interpretability when jointly account for latent relationships across multiple channels. In the latent space, this is achieved by constraining the variational distribution of each channel to a common target prior. Parsimonious latent representations are enforced by variational dropout. Experiments on synthetic data show that our model correctly identifies the prescribed latent dimensions and data relationships across multiple testing scenarios. When applied to imaging and clinical data, our method allows to identify the joint effect of age and pathology in describing clinical condition in a large scale clinical cohort.},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Antelmi, Luigi and Ayache, Nicholas and Robert, Philippe and Lorenzi, Marco},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {},
	pages = {302--311},
	file = {Full Text PDF:/Users/florencetownend/Zotero/storage/7XVLI3FP/Antelmi et al. - 2019 - Sparse Multi-Channel Variational Autoencoder for t.pdf:application/pdf;Supplementary PDF:/Users/florencetownend/Zotero/storage/RMJHDUF4/Antelmi et al. - 2019 - Sparse Multi-Channel Variational Autoencoder for t.pdf:application/pdf},
}

@misc{cuiDeepMultimodalFusion2022,
	title = {Deep {Multi}-modal {Fusion} of {Image} and {Non}-image {Data} in {Disease} {Diagnosis} and {Prognosis}: {A} {Review}},
	shorttitle = {Deep {Multi}-modal {Fusion} of {Image} and {Non}-image {Data} in {Disease} {Diagnosis} and {Prognosis}},
	url = {http://arxiv.org/abs/2203.15588},
	doi = {10.48550/arXiv.2203.15588},
	abstract = {The rapid development of diagnostic technologies in healthcare is leading to higher requirements for physicians to handle and integrate the heterogeneous, yet complementary data that are produced during routine practice. For instance, the personalized diagnosis and treatment planning for a single cancer patient relies on the various images (e.g., radiological, pathological, and camera images) and non-image data (e.g., clinical data and genomic data). However, such decision-making procedures can be subjective, qualitative, and have large inter-subject variabilities. With the recent advances in multi-modal deep learning technologies, an increasingly large number of efforts have been devoted to a key question: how do we extract and aggregate multi-modal information to ultimately provide more objective, quantitative computer-aided clinical decision making? This paper reviews the recent studies on dealing with such a question. Briefly, this review will include the (1) overview of current multi-modal learning workflows, (2) summarization of multi-modal fusion methods, (3) discussion of the performance, (4) applications in disease diagnosis and prognosis, and (5) challenges and future directions.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Cui, Can and Yang, Haichun and Wang, Yaohong and Zhao, Shilin and Asad, Zuhayr and Coburn, Lori A. and Wilson, Keith T. and Landman, Bennett A. and Huo, Yuankai},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15588 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/florencetownend/Zotero/storage/DRXGFBH9/Cui et al. - 2022 - Deep Multi-modal Fusion of Image and Non-image Dat.pdf:application/pdf;arXiv.org Snapshot:/Users/florencetownend/Zotero/storage/CJBNZ2GL/2203.html:text/html},
}

@inproceedings{duanmuPredictionPathologicalComplete2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Prediction of {Pathological} {Complete} {Response} to {Neoadjuvant} {Chemotherapy} in {Breast} {Cancer} {Using} {Deep} {Learning} with {Integrative} {Imaging}, {Molecular} and {Demographic} {Data}},
	isbn = {978-3-030-59713-9},
	doi = {10.1007/978-3-030-59713-9_24},
	abstract = {Neoadjuvant chemotherapy is widely used to reduce tumor size to make surgical excision manageable and to minimize distant metastasis. Assessing and accurately predicting pathological complete response is important in treatment planing for breast cancer patients. In this study, we propose a novel approach integrating 3D MRI imaging data, molecular data and demographic data using convolutional neural network to predict the likelihood of pathological complete response to neoadjuvant chemotherapy in breast cancer. We take post-contrast T1-weighted 3D MRI images without the need of tumor segmentation, and incorporate molecular subtypes and demographic data. In our predictive model, MRI data and non-imaging data are convolved to inform each other through interactions, instead of a concatenation of multiple data type channels. This is achieved by channel-wise multiplication of the intermediate results of imaging and non-imaging data. We use a subset of curated data from the I-SPY-1 TRIAL of 112 patients with stage 2 or 3 breast cancer with breast tumors underwent standard neoadjuvant chemotherapy. Our method yielded an accuracy of 0.83, AUC of 0.80, sensitivity of 0.68 and specificity of 0.88. Our model significantly outperforms models using imaging data only or traditional concatenation models. Our approach has the potential to aid physicians to identify patients who are likely to respond to neoadjuvant chemotherapy at diagnosis or early treatment, thus facilitate treatment planning, treatment execution, or mid-treatment adjustment.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Duanmu, Hongyi and Huang, Pauline Boning and Brahmavar, Srinidhi and Lin, Stephanie and Ren, Thomas and Kong, Jun and Wang, Fusheng and Duong, Tim Q.},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Magnetic resonance imaging, Convolutional neural network, Artificial intelligence},
	pages = {242--252},
	file = {Full Text PDF:/Users/florencetownend/Zotero/storage/X5YAPHYM/Duanmu et al. - 2020 - Prediction of Pathological Complete Response to Ne.pdf:application/pdf},
}

@article{yanRicherFusionNetwork2021,
	title = {Richer fusion network for breast cancer classification based on multimodal data},
	volume = {21},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-020-01340-6},
	doi = {10.1186/s12911-020-01340-6},
	abstract = {Deep learning algorithms significantly improve the accuracy of pathological image classification, but the accuracy of breast cancer classification using only single-mode pathological images still cannot meet the needs of clinical practice. Inspired by the real scenario of pathologists reading pathological images for diagnosis, we integrate pathological images and structured data extracted from clinical electronic medical record (EMR) to further improve the accuracy of breast cancer classification.},
	number = {1},
	urldate = {2022-10-27},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Yan, Rui and Zhang, Fa and Rao, Xiaosong and Lv, Zhilong and Li, Jintao and Zhang, Lingling and Liang, Shuang and Li, Yilin and Ren, Fei and Zheng, Chunhou and Liang, Jun},
	month = apr,
	year = {2021},
	keywords = {Convolutional neural network, Multimodal fusion, Breast cancer classification, Electronic medical record, Pathological image},
	pages = {134},
}

@article{zhaoMultimodalDeepLearning2022,
	title = {A {Multimodal} {Deep} {Learning} {Approach} to {Predicting} {Systemic} {Diseases} from {Oral} {Conditions}},
	volume = {12},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/12/12/3192},
	doi = {10.3390/diagnostics12123192},
	abstract = {Background: It is known that oral diseases such as periodontal (gum) disease are closely linked to various systemic diseases and disorders. Deep learning advances have the potential to make major contributions to healthcare, particularly in the domains that rely on medical imaging. Incorporating non-imaging information based on clinical and laboratory data may allow clinicians to make more comprehensive and accurate decisions. Methods: Here, we developed a multimodal deep learning method to predict systemic diseases and disorders from oral health conditions. A dual-loss autoencoder was used in the first phase to extract periodontal disease-related features from 1188 panoramic radiographs. Then, in the second phase, we fused the image features with the demographic data and clinical information taken from electronic health records (EHR) to predict systemic diseases. We used receiver operation characteristics (ROC) and accuracy to evaluate our model. The model was further validated by an unseen test dataset. Findings: According to our findings, the top three most accurately predicted chapters, in order, are the Chapters III, VI and IX. The results indicated that the proposed model could predict systemic diseases belonging to Chapters III, VI and IX, with AUC values of 0.92 (95\% CI, 0.90–94), 0.87 (95\% CI, 0.84–89) and 0.78 (95\% CI, 0.75–81), respectively. To assess the robustness of the models, we performed the evaluation on the unseen test dataset for these chapters and the results showed an accuracy of 0.88, 0.82 and 0.72 for Chapters III, VI and IX, respectively. Interpretation: The present study shows that the combination of panoramic radiograph and clinical oral features could be considered to train a fusion deep learning model for predicting systemic diseases and disorders.},
	language = {en},
	number = {12},
	urldate = {2023-01-26},
	journal = {Diagnostics},
	author = {Zhao, Dan and Homayounfar, Morteza and Zhen, Zhe and Wu, Mei-Zhen and Yu, Shuk Yin and Yiu, Kai-Hang and Vardhanabhuti, Varut and Pelekos, George and Jin, Lijian and Koohi-Moghadam, Mohamad},
	month = dec,
	year = {2022},
	keywords = {},
	pages = {3192},
}

@article{huangPredictingColorectalCancer2022,
	title = {Predicting colorectal cancer tumor mutational burden from histopathological images and clinical information using multi-modal deep learning},
	volume = {38},
	issn = {1367-4803, 1367-4811},
	url = {https://academic.oup.com/bioinformatics/article/38/22/5108/6709345},
	doi = {10.1093/bioinformatics/btac641},
	abstract = {Abstract
            
              Motivation
              Tumor mutational burden (TMB) is an indicator of the efficacy and prognosis of immune checkpoint therapy in colorectal cancer (CRC). In general, patients with higher TMB values are more likely to benefit from immunotherapy. Though whole-exome sequencing is considered the gold standard for determining TMB, it is difficult to be applied in clinical practice due to its high cost. There are also a few DNA panel-based methods to estimate TMB; however, their detection cost is also high, and the associated wet-lab experiments usually take days, which emphasize the need for faster and cheaper alternatives.
            
            
              Results
              In this study, we propose a multi-modal deep learning model based on a residual network (ResNet) and multi-modal compact bilinear pooling to predict TMB status (i.e. TMB high (TMB\_H) or TMB low(TMB\_L)) directly from histopathological images and clinical data. We applied the model to CRC data from The Cancer Genome Atlas and compared it with four other popular methods, namely, ResNet18, ResNet50, VGG19 and AlexNet. We tested different TMB thresholds, namely, percentiles of 10\%, 14.3\%, 15\%, 16.3\%, 20\%, 30\% and 50\%, to differentiate TMB\_H and TMB\_L.
              For the percentile of 14.3\% (i.e. TMB value 20) and ResNet18, our model achieved an area under the receiver operating characteristic curve of 0.817 after 5-fold cross-validation, which was better than that of other compared models. In addition, we also found that TMB values were significantly associated with the tumor stage and N and M stages. Our study shows that deep learning models can predict TMB status from histopathological images and clinical information only, which is worth clinical application.},
	language = {en},
	number = {22},
	urldate = {2023-01-26},
	journal = {Bioinformatics},
	author = {Huang, Kaimei and Lin, Binghu and Liu, Jinyang and Liu, Yankun and Li, Jingwu and Tian, Geng and Yang, Jialiang},
	editor = {Peng, Hanchuan},
	month = nov,
	year = {2022},
	keywords = {},
	pages = {5108--5115},
}

@article{golovanevskyMultimodalAttentionbasedDeep2022,
	title = {Multimodal attention-based deep learning for {Alzheimer}’s disease diagnosis},
	volume = {29},
	issn = {1067-5027, 1527-974X},
	url = {https://academic.oup.com/jamia/article/29/12/2014/6712292},
	doi = {10.1093/jamia/ocac168},
	abstract = {Abstract
            
              Objective
              Alzheimer’s disease (AD) is the most common neurodegenerative disorder with one of the most complex pathogeneses, making effective and clinically actionable decision support difficult. The objective of this study was to develop a novel multimodal deep learning framework to aid medical professionals in AD diagnosis.
            
            
              Materials and Methods
              We present a Multimodal Alzheimer’s Disease Diagnosis framework (MADDi) to accurately detect the presence of AD and mild cognitive impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in that we use cross-modal attention, which captures interactions between modalities—a method not previously explored in this domain. We perform multi-class classification, a challenging task considering the strong similarities between MCI and AD. We compare with previous state-of-the-art models, evaluate the importance of attention, and examine the contribution of each modality to the model’s performance.
            
            
              Results
              MADDi classifies MCI, AD, and controls with 96.88\% accuracy on a held-out test set. When examining the contribution of different attention schemes, we found that the combination of cross-modal attention with self-attention performed the best, and no attention layers in the model performed the worst, with a 7.9\% difference in F1-scores.
            
            
              Discussion
              Our experiments underlined the importance of structured clinical data to help machine learning models contextualize and interpret the remaining modalities. Extensive ablation studies showed that any multimodal mixture of input features without access to structured clinical information suffered marked performance losses.
            
            
              Conclusion
              This study demonstrates the merit of combining multiple input modalities via cross-modal attention to deliver highly accurate AD diagnostic decision support.},
	language = {en},
	number = {12},
	urldate = {2023-01-26},
	journal = {Journal of the American Medical Informatics Association},
	author = {Golovanevsky, Michal and Eickhoff, Carsten and Singh, Ritambhara},
	month = nov,
	year = {2022},
	keywords = {},
	pages = {2014--2022},
}

@article{gaoReducingUncertaintyCancer2022,
	title = {Reducing uncertainty in cancer risk estimation for patients with indeterminate pulmonary nodules using an integrated deep learning model},
	volume = {150},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482522008216},
	doi = {10.1016/j.compbiomed.2022.106113},
	language = {en},
	urldate = {2023-01-26},
	journal = {Computers in Biology and Medicine},
	author = {Gao, Riqiang and Li, Thomas and Tang, Yucheng and Xu, Kaiwen and Khan, Mirza and Kammer, Michael and Antic, Sanja L. and Deppen, Stephen and Huo, Yuankai and Lasko, Thomas A. and Sandler, Kim L. and Maldonado, Fabien and Landman, Bennett A.},
	month = nov,
	year = {2022},
	keywords = {},
	pages = {106113},
}

@inproceedings{liFusingMetadataDermoscopy2020,
	title = {Fusing {Metadata} and {Dermoscopy} {Images} for {Skin} {Disease} {Diagnosis}},
	doi = {10.1109/ISBI45749.2020.9098645},
	abstract = {To date, it is still difficult and challenging to automatically classify dermoscopy images. Although the state-of-the-art convolutional networks were applied to solve the classification problem and achieved overall decent prediction results, there is still room for performance improvement, especially for rare disease categories. Considering that human dermatologists often make use of other information (e.g., body locations of skin lesions) to help diagnose, we propose using both dermoscopy images and non-image metadata for intelligent diagnosis of skin diseases. Specifically, the metadata information is innovatively applied to control the importance of different types of visual information during diagnosis. Comprehensive experiments with various deep learning model architectures demonstrated the superior performance of the proposed fusion approach especially for relatively rare diseases. All our codes will be made publicly available.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Li, Weipeng and Zhuang, Jiaxin and Wang, Ruixuan and Zhang, Jianguo and Zheng, Wei-Shi},
	month = apr,
	year = {2020},
	note = {ISSN: 1945-8452},
	keywords = {Diseases, Feature extraction, Task analysis, Data integration, Visualization, data fusion, metadata, Metadata, Skin, Skin disease classification},
	pages = {1996--2000},
}

@misc{bintsiMultimodalBrainAge2023,
	title = {Multimodal brain age estimation using interpretable adaptive population-graph learning},
	url = {http://arxiv.org/abs/2307.04639},
	abstract = {Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Bintsi, Kyriaki-Margarita and Baltatzis, Vasileios and Potamias, Rolandos Alexandros and Hammers, Alexander and Rueckert, Daniel},
	month = jul,
	year = {2023},
	note = {arXiv:2307.04639 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chenMDFNetApplicationMultimodal2023,
	title = {{MDFNet}: application of multimodal fusion method based on skin image and clinical data to skin cancer classification},
	volume = {149},
	issn = {1432-1335},
	shorttitle = {{MDFNet}},
	url = {https://doi.org/10.1007/s00432-022-04180-1},
	doi = {10.1007/s00432-022-04180-1},
	abstract = {Skin cancer is one of the ten most common cancer types in the world. Early diagnosis and treatment can effectively reduce the mortality of patients. Therefore, it is of great significance to develop an intelligent diagnosis system for skin cancer. According to the survey, at present, most intelligent diagnosis systems of skin cancer only use skin image data, but the multi-modal cross-fusion analysis using image data and patient clinical data is limited. Therefore, to further explore the complementary relationship between image data and patient clinical data, we propose multimode data fusion diagnosis network (MDFNet), a framework for skin cancer based on data fusion strategy.},
	language = {en},
	number = {7},
	urldate = {2023-11-08},
	journal = {Journal of Cancer Research and Clinical Oncology},
	author = {Chen, Qian and Li, Min and Chen, Chen and Zhou, Panyun and Lv, Xiaoyi and Chen, Cheng},
	month = jul,
	year = {2023},
	keywords = {Clinical data, Computer-aided diagnosis, Multimodal data fusion, Skin cancer},
	pages = {3287--3299},
}

@article{perryMvlearnMultiviewMachine2021,
	title = {mvlearn: {Multiview} {Machine} {Learning} in {Python}},
	abstract = {As data are generated more and more from multiple disparate sources, multiview data sets, where each sample has features in distinct views, have grown in recent years. However, no comprehensive package exists that enables non-specialists to use these methods easily. mvlearn is a Python library which implements the leading multiview machine learning methods. Its simple API closely follows that of scikit-learn for increased ease-of-use. The package can be installed from Python Package Index (PyPI) and the conda package manager and is released under the MIT open-source license. The documentation, detailed examples, and all releases are available at https://mvlearn.github.io/.},
	language = {en},
	author = {Perry, Ronan and Mischler, Gavin and Guo, Richard and Lee, Theodore and Chang, Alexander and Koul, Arman and Franz, Cameron and Richard, Hugo and Carmichael, Iain and Ablin, Pierre and Gramfort, Alexandre and Vogelstein, Joshua T},
	month = jan,
	year = {2021},
}

@article{chapmanCCAZooCollectionRegularized2021,
	title = {{CCA}-{Zoo}: {A} collection of {Regularized}, {Deep} {Learning} based, {Kernel}, and {Probabilistic} {CCA} methods in a scikit-learn style framework},
	volume = {6},
	issn = {2475-9066},
	shorttitle = {{CCA}-{Zoo}},
	url = {https://joss.theoj.org/papers/10.21105/joss.03823},
	doi = {10.21105/joss.03823},
	abstract = {Multi-view data has gained visibility in scientific research. Examples include different languages in natural language processing, as well as neuroimaging, multiomics and audiovisual data. Canonical Correlation Analysis (CCA) (Hotelling, 1992) and Partial Least Squares (PLS) are classical methods for investigating and quantifying multivariate relationships between these views of data. The goal of CCA and its variants is to find projections (and associated weights) for each view of the data into a latent space where they are highly correlated.},
	language = {en},
	number = {68},
	urldate = {2023-12-04},
	journal = {Journal of Open Source Software},
	author = {Chapman, James and Wang, Hao-Ting},
	month = dec,
	year = {2021},
	pages = {3823},
}

@article{zaurinPytorchwidedeepFlexiblePackage2023,
	title = {pytorch-widedeep: {A} flexible package for multimodal deep learning},
	volume = {8},
	issn = {2475-9066},
	shorttitle = {pytorch-widedeep},
	url = {https://joss.theoj.org/papers/10.21105/joss.05027},
	doi = {10.21105/joss.05027},
	abstract = {Zaurin et al., (2023). pytorch-widedeep: A flexible package for multimodal deep learning. Journal of Open Source Software, 8(86), 5027, https://doi.org/10.21105/joss.05027},
	language = {en},
	number = {86},
	urldate = {2023-12-04},
	journal = {Journal of Open Source Software},
	author = {Zaurin, Javier Rodriguez and Mulinka, Pavol},
	month = jun,
	year = {2023},
	pages = {5027},
}

@article{aguilaMultiviewAEPythonPackage2023,
	title = {Multi-view-{AE}: {A} {Python} package for multi-view autoencoder models},
	volume = {8},
	issn = {2475-9066},
	shorttitle = {Multi-view-{AE}},
	url = {https://joss.theoj.org/papers/10.21105/joss.05093},
	doi = {10.21105/joss.05093},
	abstract = {Aguila et al., (2023). Multi-view-AE: A Python package for multi-view autoencoder models. Journal of Open Source Software, 8(85), 5093, https://doi.org/10.21105/joss.05093},
	language = {en},
	number = {85},
	urldate = {2023-12-04},
	journal = {Journal of Open Source Software},
	author = {Aguila, Ana Lawry and Jayme, Alejandra and Montaña-Brown, Nina and Heuveline, Vincent and Altmann, Andre},
	month = may,
	year = {2023},
	pages = {5093},
}

@misc{tianImage_tabular2020,
	title = {image\_tabular},
	url = {https://github.com/naity/image_tabular/},
	urldate = {2023-12-04},
	author = {Tian, Yuan},
	month = jul,
	year = {2020},
}

@article{lipkovaArtificialIntelligenceMultimodal2022,
	title = {Artificial intelligence for multimodal data integration in oncology},
	volume = {40},
	issn = {1535-6108},
	url = {https://www.sciencedirect.com/science/article/pii/S153561082200441X},
	doi = {10.1016/j.ccell.2022.09.012},
	abstract = {In oncology, the patient state is characterized by a whole spectrum of modalities, ranging from radiology, histology, and genomics to electronic health records. Current artificial intelligence (AI) models operate mainly in the realm of a single modality, neglecting the broader clinical context, which inevitably diminishes their potential. Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models, bringing AI closer to clinical practice. AI models are also capable of discovering novel patterns within and across modalities suitable for explaining differences in patient outcomes or treatment resistance. The insights gleaned from such models can guide exploration studies and contribute to the discovery of novel biomarkers and therapeutic targets. To support these advances, here we present a synopsis of AI methods and strategies for multimodal data fusion and association discovery. We outline approaches for AI interpretability and directions for AI-driven exploration through multimodal data interconnections. We examine challenges in clinical adoption and discuss emerging solutions.},
	number = {10},
	urldate = {2023-12-14},
	journal = {Cancer Cell},
	author = {Lipkova, Jana and Chen, Richard J. and Chen, Bowen and Lu, Ming Y. and Barbieri, Matteo and Shao, Daniel and Vaidya, Anurag J. and Chen, Chengkuan and Zhuang, Luoting and Williamson, Drew F. K. and Shaban, Muhammad and Chen, Tiffany Y. and Mahmood, Faisal},
	month = oct,
	year = {2022},
	keywords = {deep learning, AI in oncology, deep learning in oncology, multimodal AI, multimodal fusion, multimodal integration},
	pages = {1095--1110},
}

@article{luoArtificialIntelligenceassistedDermatology2023,
	title = {Artificial intelligence-assisted dermatology diagnosis: {From} unimodal to multimodal},
	volume = {165},
	issn = {0010-4825},
	shorttitle = {Artificial intelligence-assisted dermatology diagnosis},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482523008788},
	doi = {10.1016/j.compbiomed.2023.107413},
	abstract = {Artificial Intelligence (AI) is progressively permeating medicine, notably in the realm of assisted diagnosis. However, the traditional unimodal AI models, reliant on large volumes of accurately labeled data and single data type usage, prove insufficient to assist dermatological diagnosis. Augmenting these models with text data from patient narratives, laboratory reports, and image data from skin lesions, dermoscopy, and pathologies could significantly enhance their diagnostic capacity. Large-scale pre-training multimodal models offer a promising solution, exploiting the burgeoning reservoir of clinical data and amalgamating various data types. This paper delves into unimodal models’ methodologies, applications, and shortcomings while exploring how multimodal models can enhance accuracy and reliability. Furthermore, integrating cutting-edge technologies like federated learning and multi-party privacy computing with AI can substantially mitigate patient privacy concerns in dermatological datasets and further fosters a move towards high-precision self-diagnosis. Diagnostic systems underpinned by large-scale pre-training multimodal models can facilitate dermatology physicians in formulating effective diagnostic and treatment strategies and herald a transformative era in healthcare.},
	urldate = {2023-12-14},
	journal = {Computers in Biology and Medicine},
	author = {Luo, Nan and Zhong, Xiaojing and Su, Luxin and Cheng, Zilin and Ma, Wenyi and Hao, Pingsheng},
	month = oct,
	year = {2023},
	keywords = {Machine learning, Multimodal, AI-assisted diagnosis, Dermatology, Federated learning, Pre-training},
	pages = {107413},
}

@article{huangMultimodalLearningClinically2023,
	title = {Multimodal learning of clinically accessible tests to aid diagnosis of neurodegenerative disorders: a scoping review},
	volume = {11},
	issn = {2047-2501},
	shorttitle = {Multimodal learning of clinically accessible tests to aid diagnosis of neurodegenerative disorders},
	url = {https://doi.org/10.1007/s13755-023-00231-0},
	doi = {10.1007/s13755-023-00231-0},
	abstract = {With ageing populations around the world, there is a rapid rise in the number of people with Alzheimer’s disease (AD) and Parkinson’s disease (PD), the two most common types of neurodegenerative disorders. There is an urgent need to find new ways of aiding early diagnosis of these conditions. Multimodal learning of clinically accessible data is a relatively new approach that holds great potential to support early precise diagnosis. This scoping review follows the PRSIMA guidelines and we analysed 46 papers, comprising 11,750 participants, 3569 with AD, 978 with PD, and 2482 healthy controls; the recency of this topic was highlighted by nearly all papers being published in the last 5 years. It highlights the effectiveness of combining different types of data, such as brain scans, cognitive scores, speech and language, gait, hand and eye movements, and genetic assessments for the early detection of AD and PD. The review also outlines the AI methods and the model used in each study, which includes feature extraction, feature selection, feature fusion, and using multi-source discriminative features for classification. The review identifies knowledge gaps around the need to validate findings and address limitations such as small sample sizes. Applying multimodal learning of clinically accessible tests holds strong potential to aid the development of low-cost, reliable, and non-invasive methods for early detection of AD and PD.},
	language = {en},
	number = {1},
	urldate = {2023-12-14},
	journal = {Health Information Science and Systems},
	author = {Huang, Guan and Li, Renjie and Bai, Quan and Alty, Jane},
	month = jul,
	year = {2023},
	keywords = {Alzheimer’s disease, Artificial intelligence, Parkinson’s disease, Multimodal learning, Age-related diseases, Diagnosis, Multiple biomarkers, Pre-clinical},
	pages = {32},
}

@article{s.s.gopiMultimodalMachineLearning2023,
	title = {Multimodal {Machine} {Learning} {Based} {Crop} {Recommendation} and {Yield} {Prediction} {Model}},
	volume = {36},
	issn = {1079-8587},
	url = {https://www.techscience.com/iasc/v36n1/49968},
	doi = {10.32604/iasc.2023.029756},
	abstract = {Agriculture plays a vital role in the Indian economy. Crop recommendation for a speciﬁc region is a tedious process as it can be affected by various variables such as soil type and climatic parameters. At the same time, crop yield prediction was based on several features like area, irrigation type, temperature, etc. The recent advancements of artiﬁcial intelligence (AI) and machine learning (ML) models pave the way to design effective crop recommendation and crop prediction models. In this view, this paper presents a novel Multimodal Machine Learning Based Crop Recommendation and Yield Prediction (MMML-CRYP) technique. The proposed MMML-CRYP model mainly focuses on two processes namely crop recommendation and crop prediction. At the initial stage, equilibrium optimizer (EO) with kernel extreme learning machine (KELM) technique is employed for effectual recommendation of crops. Next, random forest (RF) technique was executed for predicting the crop yield accurately. For reporting the improved performance of the MMML-CRYP system, a wide range of simulations were carried out and the results are investigated using benchmark dataset. Experimentation outcomes highlighted the signiﬁcant performance of the MMMLCRYP approach on the compared approaches with maximum accuracy of 97.91\%.},
	language = {en},
	number = {1},
	urldate = {2023-12-14},
	journal = {Intelligent Automation \& Soft Computing},
	author = {S. S. Gopi, P. and Karthikeyan, M.},
	year = {2023},
	pages = {313--326},
}

@article{algiriyageMultisourceMultimodalData2021,
	title = {Multi-source {Multimodal} {Data} and {Deep} {Learning} for {Disaster} {Response}: {A} {Systematic} {Review}},
	volume = {3},
	issn = {2661-8907},
	shorttitle = {Multi-source {Multimodal} {Data} and {Deep} {Learning} for {Disaster} {Response}},
	url = {https://doi.org/10.1007/s42979-021-00971-4},
	doi = {10.1007/s42979-021-00971-4},
	abstract = {Mechanisms for sharing information in a disaster situation have drastically changed due to new technological innovations throughout the world. The use of social media applications and collaborative technologies for information sharing have become increasingly popular. With these advancements, the amount of data collected increases daily in different modalities, such as text, audio, video, and images. However, to date, practical Disaster Response (DR) activities are mostly depended on textual information, such as situation reports and email content, and the benefit of other media is often not realised. Deep Learning (DL) algorithms have recently demonstrated promising results in extracting knowledge from multiple modalities of data, but the use of DL approaches for DR tasks has thus far mostly been pursued in an academic context. This paper conducts a systematic review of 83 articles to identify the successes, current and future challenges, and opportunities in using DL for DR tasks. Our analysis is centred around the components of learning, a set of aspects that govern the application of Machine learning (ML) for a given problem domain. A flowchart and guidance for future research are developed as an outcome of the analysis to ensure the benefits of DL for DR activities are utilized.},
	language = {en},
	number = {1},
	urldate = {2023-12-14},
	journal = {SN Computer Science},
	author = {Algiriyage, Nilani and Prasanna, Raj and Stock, Kristin and Doyle, Emma E. H. and Johnston, David},
	month = nov,
	year = {2021},
	keywords = {Deep learning, Disaster management, Disaster response, Literature review},
	pages = {92},
}

@article{duanMultimodalSensorsMLBased2022,
	title = {Multimodal {Sensors} and {ML}-{Based} {Data} {Fusion} for {Advanced} {Robots}},
	volume = {4},
	copyright = {© 2022 The Authors. Advanced Intelligent Systems published by Wiley-VCH GmbH},
	issn = {2640-4567},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202200213},
	doi = {10.1002/aisy.202200213},
	abstract = {Nature has proved that multiple sense and processing capabilities are critical for target recognition and race survival. As such, advanced robots that perform missions in an unstructured environment highly need organism-similar multimodal sensing and processing systems for sophisticated unstructured environmental stimuli. Herein, recent progress in multimodal sensing and processing systems for advanced robotics is reviewed. Multimodal sensors including tactile sensors that capture surface properties of objects (i.e., thermal conductivity, temperature, softness, and electron affinity), visual sensors that capture the color and size of objects, and gas sensors that capture object smell are summarized. The multimodal data fusion algorithms that process multimodal signals from multimodal sensors to achieve object recognition and decision making are also presented. The challenges and future development of multimodal sensors and data fusion algorithms are further discussed. Advances in these areas open new avenues for advanced robotics applications in human–robot collaboration, rescue missions, garbage sorting, and intelligent prosthetics.},
	language = {en},
	number = {12},
	urldate = {2023-12-14},
	journal = {Advanced Intelligent Systems},
	author = {Duan, Shengshun and Shi, Qiongfeng and Wu, Jun},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202200213},
	keywords = {multimodal, machine learning, electronic skin, robotics, tactile sensors},
	pages = {2200213},
}

@article{jiangSnapshotResearchImplementation2020,
	title = {A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition},
	volume = {53},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519301381},
	doi = {10.1016/j.inffus.2019.06.019},
	abstract = {With the rapid development of artificial intelligence and mobile Internet, the new requirements for human-computer interaction have been put forward. The personalized emotional interaction service is a new trend in the human-computer interaction field. As a basis of emotional interaction, emotion recognition has also introduced many new advances with the development of artificial intelligence. The current research on emotion recognition mostly focuses on single-modal recognition such as expression recognition, speech recognition, limb recognition, and physiological signal recognition. However, the lack of the single-modal emotional information and vulnerability to various external factors lead to lower accuracy of emotion recognition. Therefore, multimodal information fusion for data-driven emotion recognition has been attracting the attention of researchers in the affective computing filed. This paper reviews the development background and hot spots of the data-driven multimodal emotion information fusion. Considering the real-time mental health monitoring system, the current development of multimodal emotion data sets, the multimodal features extraction, including the EEG, speech, expression, text features, and multimodal fusion strategies and recognition methods are discussed and summarized in detail. The main objective of this work is to present a clear explanation of the scientific problems and future research directions in the multimodal information fusion for data-driven emotion recognition field.},
	urldate = {2023-12-14},
	journal = {Information Fusion},
	author = {Jiang, Yingying and Li, Wei and Hossain, M. Shamim and Chen, Min and Alelaiwi, Abdulhameed and Al-Hammadi, Muneer},
	month = jan,
	year = {2020},
	keywords = {Artificial intelligence, Data-driven emotion recognition, Multimodal information fusion},
	pages = {209--221},
}

@article{gandhiMultimodalSentimentAnalysis2023,
	title = {Multimodal sentiment analysis: {A} systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
	volume = {91},
	issn = {1566-2535},
	shorttitle = {Multimodal sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
	doi = {10.1016/j.inffus.2022.09.025},
	abstract = {Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for multimodal feature extraction and fusion and sentiment polarity detection, with aims to minimize error rate and improve performance. This survey paper examines primary taxonomy and newly released multimodal fusion architectures. Recent developments in MSA architectures are divided into ten categories, namely early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion. A comparison of several architectural evolutions in terms of MSA fusion categories and their relative strengths and limitations are presented. Finally, a number of interdisciplinary applications and future research directions are proposed.},
	urldate = {2023-12-14},
	journal = {Information Fusion},
	author = {Gandhi, Ankita and Adhvaryu, Kinjal and Poria, Soujanya and Cambria, Erik and Hussain, Amir},
	month = mar,
	year = {2023},
	keywords = {Multimodal fusion, Affective computing, Fusion techniques, Sentiment analysis},
	pages = {424--444},
}

@article{patilRiceFusionMultimodalityData2022,
	title = {Rice-{Fusion}: {A} {Multimodality} {Data} {Fusion} {Framework} for {Rice} {Disease} {Diagnosis}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Rice-{Fusion}},
	url = {https://ieeexplore.ieee.org/document/9672157/},
	doi = {10.1109/ACCESS.2022.3140815},
	abstract = {Rice leaf infections are a common hazard to rice production, affecting many farmers all over the world. Early detection and treatment of rice leaf infection are critical for promoting healthy rice plant growth and ensuring adequate supply for the fast-growing population. Computer-assisted rice leaf disease diagnoses are hampered due to strong image backgrounds. Popular Convolutional Neural Network (CNN) architecture extracts the features from images and diagnoses the disease to address the issues above. However, this method is best suitable for segmented images and gives low accuracy with real-time images. In this case, the Internet of Things is a paradigm shift that collects agro-meteorological information that effectively helps diagnose rice diseases. Motivated by the usefulness of CNN models and agricultural IoT, a novel multimodal data fusion framework named Rice-Fusion is proposed to diagnose rice disease. Rice disease diagnosis based on a single modality may not be accurate, and hence the fusion of heterogeneous modalities is essential for robust and reliable disease diagnosis. This gives a new dimension to the domain of rice disease diagnosis. The dataset was collected manually with 3200 rice health category samples using two modalities, namely agro-meteorological sensors and a camera. The Rice-Fusion framework initially extracts the numerical features from agro-meteorological data collected from sensors. Next, it extracts the visual features from the captured rice images. These extracted features are further fused using a concatenation layer followed by a dense layer, which provides single output for diagnosing the rice disease. The testing accuracy of Rice-Fusion is 95.31\% as opposed to other unimodal framework accuracies of 82.03\% and 91.25\% based on CNN and Multi-Layer Perceptron (MLP) architectures, respectively. Experimental results analysis demonstrates that the proposed Rice-Fusion multimodal data fusion framework outperforms the outcome of unimodal frameworks.},
	language = {en},
	urldate = {2023-12-14},
	journal = {IEEE Access},
	author = {Patil, Rutuja R. and Kumar, Sumit},
	year = {2022},
	pages = {5207--5222},
}

@article{yanDeepMultiviewLearning2021a,
	title = {Deep multi-view learning methods: {A} review},
	volume = {448},
	issn = {0925-2312},
	shorttitle = {Deep multi-view learning methods},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221004768},
	doi = {10.1016/j.neucom.2021.03.090},
	abstract = {Multi-view learning (MVL) has attracted increasing attention and achieved great practical success by exploiting complementary information of multiple features or modalities. Recently, due to the remarkable performance of deep models, deep MVL has been adopted in many domains, such as machine learning, artificial intelligence and computer vision. This paper presents a comprehensive review on deep MVL from the following two perspectives: MVL methods in deep learning scope and deep MVL extensions of traditional methods. Specifically, we first review the representative MVL methods in the scope of deep learning, such as multi-view auto-encoder, conventional neural networks and deep brief networks. Then, we investigate the advancements of the MVL mechanism when traditional learning methods meet deep learning models, such as deep multi-view canonical correlation analysis, matrix factorization and information bottleneck. Moreover, we also summarize the main applications, widely-used datasets and performance comparison in the domain of deep MVL. Finally, we attempt to identify some open challenges to inform future research directions.},
	urldate = {2023-12-14},
	journal = {Neurocomputing},
	author = {Yan, Xiaoqiang and Hu, Shizhe and Mao, Yiqiao and Ye, Yangdong and Yu, Hui},
	month = aug,
	year = {2021},
	keywords = {Deep multi-view learning, deep neural networks, representation learning, statistical learning survey},
	pages = {106--129},
}

@article{gaoSurveyDeepLearning2020,
	title = {A {Survey} on {Deep} {Learning} for {Multimodal} {Data} {Fusion}},
	volume = {32},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01273},
	doi = {10.1162/neco_a_01273},
	abstract = {With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described.},
	number = {5},
	urldate = {2023-12-14},
	journal = {Neural Computation},
	author = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
	month = may,
	year = {2020},
	pages = {829--864},
}

@article{stahlschmidtMultimodalDeepLearning2022,
	title = {Multimodal deep learning for biomedical data fusion: a review},
	volume = {23},
	issn = {1477-4054},
	shorttitle = {Multimodal deep learning for biomedical data fusion},
	url = {https://doi.org/10.1093/bib/bbab569},
	doi = {10.1093/bib/bbab569},
	abstract = {Biomedical data are becoming increasingly multimodal and thereby capture the underlying complex relationships among biological processes. Deep learning (DL)-based data fusion strategies are a popular approach for modeling these nonlinear relationships. Therefore, we review the current state-of-the-art of such methods and propose a detailed taxonomy that facilitates more informed choices of fusion strategies for biomedical applications, as well as research on novel methods. By doing so, we find that deep fusion strategies often outperform unimodal and shallow approaches. Additionally, the proposed subcategories of fusion strategies show different advantages and drawbacks. The review of current methods has shown that, especially for intermediate fusion strategies, joint representation learning is the preferred approach as it effectively models the complex interactions of different levels of biological organization. Finally, we note that gradual fusion, based on prior biological knowledge or on search strategies, is a promising future research path. Similarly, utilizing transfer learning might overcome sample size limitations of multimodal data sets. As these data sets become increasingly available, multimodal DL approaches present the opportunity to train holistic models that can learn the complex regulatory dynamics behind health and disease.},
	number = {2},
	urldate = {2023-12-14},
	journal = {Briefings in Bioinformatics},
	author = {Stahlschmidt, Sören Richard and Ulfenborg, Benjamin and Synnergren, Jane},
	month = mar,
	year = {2022},
	pages = {bbab569},
}

@misc{zitnikScikitfusion2015,
	title = {scikit-fusion},
	url = {https://github.com/mims-harvard/scikit-fusion/tree/master},
	urldate = {2023-12-05},
	author = {Zitnik, Marinka},
	year = {2015},
}
