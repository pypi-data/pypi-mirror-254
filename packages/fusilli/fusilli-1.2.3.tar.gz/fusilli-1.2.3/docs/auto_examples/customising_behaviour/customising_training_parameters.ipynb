{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to customise the training in Fusilli\n\nThis tutorial will show you how to customise the training of your fusion model.\n\nWe will cover the following topics:\n\n* Early stopping\n* Batch size\n* Number of epochs\n* Checkpoint suffix modification\n\n## Early stopping\n\nEarly stopping is implemented in Fusilli using the PyTorch Lightning\n[EarlyStopping](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping)\ncallback. This callback can be passed to the\n:func:`~fusilli.model_utils.train_and_save_models` function using the\n``early_stopping_callback`` argument. For example:\n\n```python\nfrom fusilli.data import prepare_fusion_data\nfrom fusilli.train import train_and_save_models\n\nfrom lightning.pytorch.callbacks import EarlyStopping\n\nmodified_early_stopping_callback = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.00,\n    patience=3,\n    verbose=True,\n    mode=\"min\",\n)\n\ndatamodule = prepare_fusion_data(\n        prediction_task=\"binanry\",\n        fusion_model=example_model,\n        data_paths=data_paths,\n        output_paths=output_path,\n        own_early_stopping_callback=modified_early_stopping_callback,\n    )\n\ntrained_model_list = train_and_save_models(\n    data_module=datamodule,\n    fusion_model=example_model,\n    )\n```\nNote that you only need to pass the callback to the :func:`~.fusilli.data.prepare_fusion_data` and **not** to the :func:`~.fusilli.train.train_and_save_models` function. The new early stopping measure will be saved within the data module and accessed during training.\n\n\n-----\n\n## Batch size\n\nThe batch size can be set using the ``batch_size`` argument in the :func:`~.fusilli.data.prepare_fusion_data` function. By default, the batch size is 8.\n\n```python\nfrom fusilli.data import prepare_fusion_data\nfrom fusilli.train import train_and_save_models\n\ndatamodule = prepare_fusion_data(\n        prediction_task=\"binary\",\n        fusion_model=example_model,\n        data_paths=data_paths,\n        output_paths=output_path,\n        batch_size=32\n    )\n\ntrained_model_list = train_and_save_models(\n        data_module=datamodule,\n        fusion_model=example_model,\n        batch_size=32,\n    )\n```\n-----\n\n## Number of epochs\n\nYou can change the maximum number of epochs using the ``max_epochs`` argument in the :func:`~.fusilli.data.prepare_fusion_data` and :func:`~.fusilli.train.train_and_save_models` functions. By default, the maximum number of epochs is 1000.\n\nYou also pass it to the :func:`~.fusilli.data.prepare_fusion_data` function because some of the fusion models require pre-training.\n\nChanging the ``max_epochs`` parameter is especially useful when wanting to run a quick test of your model. For example, you can set ``max_epochs=5`` to run a quick test of your model.\n\n```python\nfrom fusilli.data import prepare_fusion_data\nfrom fusilli.train import train_and_save_models\n\ndatamodule = prepare_fusion_data(\n        prediction_task=\"binary\",\n        fusion_model=example_model,\n        data_paths=data_paths,\n        output_paths=output_path,\n        max_epochs=5,\n    )\n\ntrained_model_list = train_and_save_models(\n        data_module=datamodule,\n        fusion_model=example_model,\n        max_epochs=5,\n    )\n```\nSetting ``max_epochs`` to -1 will train the model until early stopping is triggered.\n\n-----\n\n## Checkpoint suffix modification\n\nBy default, Fusilli saves the model checkpoints in the following format:\n\n    ``{fusion_model.__name__}_epoch={epoch_n}.ckpt``\n\nIf the checkpoint is for a pre-trained model, then the following format is used:\n\n    ``subspace_{fusion_model.__name__}_{pretrained_model.__name__}.ckpt``\n\nYou can add suffixes to the checkpoint names by passing a string to the ``extra_log_string_dict`` argument in the :func:`~.fusilli.data.prepare_fusion_data` and :func:`~.fusilli.train.train_and_save_models` functions. For example, I could add a suffix to denote that I've changed the batch size for this particular run:\n\n```python\nfrom fusilli.data import prepare_fusion_data\nfrom fusilli.train import train_and_save_models\n\nextra_suffix_dict = {\"batchsize\": 32}\n\ndatamodule = prepare_fusion_data(\n        prediction_task=\"binary\",\n        fusion_model=example_model,\n        data_paths=data_paths,\n        output_paths=output_path,\n        batch_size=32,\n        extra_log_string_dict=extra_suffix_dict,\n    )\n\ntrained_model_list = train_and_save_models(\n        data_module=datamodule,\n        fusion_model=example_model,\n        batch_size=32,\n        extra_log_string_dict=extra_suffix_dict,\n    )\n```\nThe checkpoint name would then be (if the model trained for 100 epochs):\n\n    ``ExampleModel_epoch=100_batchsize_32.ckpt``\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The ``extra_log_string_dict`` argument is also used to modify the logging behaviour of the model. For more information, see `wandb`.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '_static/pink_pasta_logo.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}