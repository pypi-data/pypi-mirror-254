
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/customising_behaviour/plot_modify_layer_sizes.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_customising_behaviour_plot_modify_layer_sizes.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_customising_behaviour_plot_modify_layer_sizes.py:


How to modify fusion model architecture
################################################

This tutorial will show you how to modify the architectures of fusion models.

More guidance on what can be modified in each fusion model can be found in the :ref:`modifying-models` section.

.. warning::

    Some of the fusion models have been designed to work with specific architectures and there are some restrictions on how they can be modified.

    For example, the channel-wise attention model requires the two modalities to have the same number of layers. Please read the notes section of the fusion model you are interested in to see if there are any restrictions.

.. GENERATED FROM PYTHON SOURCE LINES 18-25

Setting up the experiment
-------------------------

First, we will set up the experiment by importing the necessary packages, creating the simulated data, and setting the parameters for the experiment.

For a more detailed explanation of this process, please see the example tutorials.


.. GENERATED FROM PYTHON SOURCE LINES 25-68

.. code-block:: Python


    # sphinx_gallery_thumbnail_path = '_static/modify_thumbnail.png'
    import matplotlib.pyplot as plt
    import os
    import torch.nn as nn
    from torch_geometric.nn import GCNConv, ChebConv

    from docs.examples import generate_sklearn_simulated_data
    from fusilli.data import prepare_fusion_data
    from fusilli.eval import RealsVsPreds
    from fusilli.train import train_and_save_models

    from fusilli.fusionmodels.tabularfusion.attention_weighted_GNN import AttentionWeightedGNN

    prediction_task = "regression"

    output_paths = {
        "checkpoints": "checkpoints",
        "losses": "loss_logs/modify_layers",
        "figures": "loss_figures",
    }

    for dir in output_paths.values():
        os.makedirs(dir, exist_ok=True)

    # empty the loss log directory (only needed for this tutorial)
    for dir in os.listdir(output_paths["losses"]):
        for file in os.listdir(os.path.join(output_paths["losses"], dir)):
            os.remove(os.path.join(output_paths["losses"], dir, file))
        # remove dir
        os.rmdir(os.path.join(output_paths["losses"], dir))

    tabular1_path, tabular2_path = generate_sklearn_simulated_data(prediction_task,
                                                                   num_samples=500,
                                                                   num_tab1_features=10,
                                                                   num_tab2_features=20)

    data_paths = {
        "tabular1": tabular1_path,
        "tabular2": tabular2_path,
        "image": "",
    }








.. GENERATED FROM PYTHON SOURCE LINES 69-114

Specifying the model modifications
----------------------------------

Now, we will specify the modifications we want to make to the model.

We are using the :class:`~fusilli.fusionmodels.tabularfusion.attention_weighted_GNN.AttentionWeightedGNN` model for this example.
This is a graph-based model which has a pretrained MLP (multi-layer perceptron) to get attention weights, and a graph neural network that uses the attention weights to perform the fusion.

The following modifications can be made to the method that makes the graph structure: :class:`~fusilli.fusionmodels.tabularfusion.attention_weighted_GNN.AttentionWeightedGraphMaker`:


.. list-table::
  :widths: 40 60
  :header-rows: 1
  :stub-columns: 0

  * - Attribute
    - Guidance
  * - :attr:`~.AttentionWeightedGraphMaker.early_stop_callback`
    - ``EarlyStopping`` object from ``from lightning.pytorch.callbacks import EarlyStopping``
  * - :attr:`~.AttentionWeightedGraphMaker.edge_probability_threshold`
    - Integer between 0 and 100.
  * - :attr:`~.AttentionWeightedGraphMaker.attention_MLP_test_size`
    - Float between 0 and 1.
  * - :attr:`~.AttentionWeightedGraphMaker.AttentionWeightingMLPInstance.weighting_layers`
    - ``nn.ModuleDict``: final layer output size must be the same as the input layer input size.
  * - :attr:`~.AttentionWeightedGraphMaker.AttentionWeightingMLPInstance.fused_layers`
    - ``nn.Sequential``


The following modifications can be made to the **fusion** model :class:`~fusilli.fusionmodels.tabularfusion.attention_weighted_GNN.AttentionWeightedGNN`:

.. list-table::
  :widths: 40 60
  :header-rows: 1
  :stub-columns: 0

  * - Attribute
    - Guidance
  * - :attr:`~.AttentionWeightedGNN.graph_conv_layers`
    - ``nn.Sequential`` of ``torch_geometric.nn`` Layers.
  * - :attr:`~.AttentionWeightedGNN.dropout_prob`
    - Float between (not including) 0 and 1.

Let's modify the model! More info about how to do this can be found in :ref:`modifying-models`.

.. GENERATED FROM PYTHON SOURCE LINES 114-148

.. code-block:: Python


    layer_mods = {
        "AttentionWeightedGNN": {
            "graph_conv_layers": nn.Sequential(
                ChebConv(15, 50, K=3),
                ChebConv(50, 100, K=3),
                ChebConv(100, 130, K=3),
            ),
            "dropout_prob": 0.4,
        },
        "AttentionWeightedGraphMaker": {
            "edge_probability_threshold": 80,
            "attention_MLP_test_size": 0.3,
            "AttentionWeightingMLPInstance.weighting_layers": nn.ModuleDict(
                {
                    "Layer 1": nn.Sequential(
                        nn.Linear(25, 100),
                        nn.ReLU()),
                    "Layer 2": nn.Sequential(
                        nn.Linear(100, 75),
                        nn.ReLU()),
                    "Layer 3": nn.Sequential(
                        nn.Linear(75, 75),
                        nn.ReLU()),
                    "Layer 4": nn.Sequential(
                        nn.Linear(75, 100),
                        nn.ReLU()),
                    "Layer 5": nn.Sequential(
                        nn.Linear(100, 30),
                        nn.ReLU()),
                }
            )},
    }








.. GENERATED FROM PYTHON SOURCE LINES 149-151

Loading the data and training the model
---------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 151-169

.. code-block:: Python



    # load data
    datamodule = prepare_fusion_data(prediction_task=prediction_task,
                                     fusion_model=AttentionWeightedGNN,
                                     data_paths=data_paths,
                                     output_paths=output_paths,
                                     layer_mods=layer_mods,
                                     max_epochs=5)

    # train
    trained_model_list = train_and_save_models(
        data_module=datamodule,
        fusion_model=AttentionWeightedGNN,
        layer_mods=layer_mods,
        max_epochs=5,
    )



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/docs/examples/customising_behaviour/plot_modify_layer_sizes.py", line 154, in <module>
        datamodule = prepare_fusion_data(prediction_task=prediction_task,
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/data.py", line 1460, in prepare_fusion_data
        graph_data_module.setup()
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/data.py", line 1137, in setup
        self.graph_data = self.graph_maker_instance.make_graph()
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/fusionmodels/tabularfusion/attention_weighted_GNN.py", line 374, in make_graph
        self.trainer.fit(self.AttentionWeightingMLPInstance, train_dataloader, val_dataloader)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
        call._call_and_handle_interrupt(
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
        return trainer_fn(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
        self._run(model, ckpt_path=ckpt_path)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 989, in _run
        results = self._run_stage()
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1035, in _run_stage
        self.fit_loop.run()
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 202, in run
        self.advance()
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 359, in advance
        self.epoch_loop.run(self._data_fetcher)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 136, in run
        self.advance(data_fetcher)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 240, in advance
        batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 187, in run
        self._optimizer_step(batch_idx, closure)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 265, in _optimizer_step
        call._call_lightning_module_hook(
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
        output = fn(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/core/module.py", line 1282, in optimizer_step
        optimizer.step(closure=optimizer_closure)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py", line 151, in step
        step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py", line 230, in optimizer_step
        return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py", line 117, in optimizer_step
        return optimizer.step(closure=closure, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/optim/optimizer.py", line 373, in wrapper
        out = func(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
        ret = func(self, *args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/optim/adam.py", line 143, in step
        loss = closure()
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py", line 104, in _wrap_closure
        closure_result = closure()
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
        self._result = self.closure(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
        return func(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
        step_output = self._step_fn()
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 315, in _training_step
        training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
        output = fn(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py", line 382, in training_step
        return self.lightning_module.training_step(*args, **kwargs)
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/fusionmodels/tabularfusion/attention_weighted_GNN.py", line 164, in training_step
        y_hat, weights = self.forward((x1, x2))
      File "/Users/florencetownend/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Projects/fusilli/fusilli/fusionmodels/tabularfusion/attention_weighted_GNN.py", line 135, in forward
        x = layer(x)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
        return self._call_impl(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
        return forward_call(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/nn/modules/container.py", line 215, in forward
        input = module(input)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
        return self._call_impl(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
        return forward_call(*args, **kwargs)
      File "/Users/florencetownend/miniforge3/envs/fusion_eval/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
        return F.linear(input, self.weight, self.bias)
    RuntimeError: linear(): input and weight.T shapes cannot be multiplied (32x30 and 25x100)




.. GENERATED FROM PYTHON SOURCE LINES 170-171

It worked! Let's have a look at the model structure to see what changes have been made.

.. GENERATED FROM PYTHON SOURCE LINES 171-175

.. code-block:: Python


    print("Attention Weighted MLP:\n", datamodule.graph_maker_instance.AttentionWeightingMLPInstance)
    print("Fusion model:\n", trained_model_list[0].model)


.. GENERATED FROM PYTHON SOURCE LINES 176-183

You can see that the input features to the ``final_prediction`` layer changed to fit with our modification to the ``graph_conv_layers`` output features!

What happens when the modifications are incorrect?
----------------------------------------------------

Let's see what happens when we try to modify an **attribute that doesn't exist**.


.. GENERATED FROM PYTHON SOURCE LINES 183-203

.. code-block:: Python


    layer_mods = {
        "AttentionWeightedGraphMaker": {
            "AttentionWeightingMLPInstance.fake_attribute": nn.Sequential(
                nn.Linear(25, 100),
                nn.ReLU(),
            ),
        }
    }

    try:
        datamodule = prepare_fusion_data(prediction_task=prediction_task,
                                         fusion_model=AttentionWeightedGNN,
                                         data_paths=data_paths,
                                         output_paths=output_paths,
                                         layer_mods=layer_mods,
                                         max_epochs=5)
    except Exception as error:
        print(error)


.. GENERATED FROM PYTHON SOURCE LINES 204-210

What about modifying an attribute with the **wrong data type**?

* ``dropout_prob`` should be an ``float`` and between 0 and 1.
* ``graph_conv_layers`` should be an ``nn.Sequential`` of graph convolutional layers.
* ``edge_probability_threshold`` should be a ``float`` between 0 and 100.
* ``AttentionWeightingMLPInstance.weighting_layers`` should be an ``nn.ModuleDict``

.. GENERATED FROM PYTHON SOURCE LINES 210-234

.. code-block:: Python


    layer_mods = {
        "AttentionWeightedGraphMaker": {
            "AttentionWeightingMLPInstance.weighting_layers": nn.Sequential(
                nn.Linear(25, 75),
                nn.ReLU(),
                nn.Linear(75, 75),
                nn.ReLU(),
                nn.Linear(75, 25),
                nn.ReLU()
            ),
        }
    }

    try:
        prepare_fusion_data(prediction_task=prediction_task,
                            fusion_model=AttentionWeightedGNN,
                            data_paths=data_paths,
                            output_paths=output_paths,
                            layer_mods=layer_mods,
                            max_epochs=5)
    except Exception as error:
        print(error)


.. GENERATED FROM PYTHON SOURCE LINES 235-252

.. code-block:: Python


    layer_mods = {
        "AttentionWeightedGraphMaker": {
            "edge_probability_threshold": "two",
        }
    }

    try:
        prepare_fusion_data(prediction_task=prediction_task,
                            fusion_model=AttentionWeightedGNN,
                            data_paths=data_paths,
                            output_paths=output_paths,
                            layer_mods=layer_mods,
                            max_epochs=5)
    except Exception as error:
        print(error)


.. GENERATED FROM PYTHON SOURCE LINES 253-265

What about modifying multiple attributes with the **conflicting modifications**?
-------------------------------------------------------------------------------------


For this, let's switch to looking at the :class:`~fusilli.fusionmodels.tabularfusion.concat_feature_maps.ConcatTabularFeatureMaps` model.
This model concatenates the feature maps of the two modalities and then passes them through a prediction layer.

We can modify the layers that each tabular modality goes through before being concatenated, as well as the layers that come after the concatenation.

The output features of our modified ``mod1_layers`` and ``mod2_layers`` are 100 and 128, so the input features of the ``fused_layers`` should be 228. However, we've set the input features of the ``fused_layers`` to be 25.

Let's see what happens when we try to modify the model in this way. It should throw an error when the data is passed through the model.

.. GENERATED FROM PYTHON SOURCE LINES 265-328

.. code-block:: Python


    layer_mods = {
        "ConcatTabularFeatureMaps": {
            "mod1_layers": nn.ModuleDict(
                {
                    "layer 1": nn.Sequential(
                        nn.Linear(10, 32),
                        nn.ReLU(),
                    ),
                    "layer 2": nn.Sequential(
                        nn.Linear(32, 66),
                        nn.ReLU(),
                    ),
                    "layer 3": nn.Sequential(
                        nn.Linear(66, 128),
                        nn.ReLU(),
                    ),
                }
            ),
            "mod2_layers": nn.ModuleDict(
                {
                    "layer 1": nn.Sequential(
                        nn.Linear(15, 45),
                        nn.ReLU(),
                    ),
                    "layer 2": nn.Sequential(
                        nn.Linear(45, 70),
                        nn.ReLU(),
                    ),
                    "layer 3": nn.Sequential(
                        nn.Linear(70, 100),
                        nn.ReLU(),
                    ),
                }
            ),
            "fused_layers": nn.Sequential(
                nn.Linear(25, 150),
                nn.ReLU(),
                nn.Linear(150, 75),
                nn.ReLU(),
                nn.Linear(75, 50),
                nn.ReLU(),
            ),
        },
    }

    # get the data and train the model

    from fusilli.fusionmodels.tabularfusion.concat_feature_maps import ConcatTabularFeatureMaps

    datamodule = prepare_fusion_data(prediction_task=prediction_task,
                                     fusion_model=ConcatTabularFeatureMaps,
                                     data_paths=data_paths,
                                     output_paths=output_paths,
                                     layer_mods=layer_mods,
                                     max_epochs=5)
    trained_model_list = train_and_save_models(
        data_module=datamodule,
        fusion_model=ConcatTabularFeatureMaps,
        layer_mods=layer_mods,
        max_epochs=5,
    )


.. GENERATED FROM PYTHON SOURCE LINES 329-331

**Wow it still works!**
Let's have a look at what the model structure looks like to see what changes have been made to keep the model valid.

.. GENERATED FROM PYTHON SOURCE LINES 331-334

.. code-block:: Python


    print(trained_model_list[0].model)


.. GENERATED FROM PYTHON SOURCE LINES 335-344

As you can see, a few corrections have been made to the modifications:

* The ``fused_layers`` has been modified to have the correct number of nodes in the first layer to match the concatenated feature maps from the two modalities.

In general, there are checks in the fusion models to make sure that the modifications are valid.
If the input number of nodes to a modification is not correct, then the model will automatically calculate the correct number of nodes and correct the modification.

This is the case for quite a few modifications, but potentially not all of them so please be careful!
Make sure to print out the model structure to check that the modifications have been made correctly and see what changes have been made to keep the model valid.

.. GENERATED FROM PYTHON SOURCE LINES 344-350

.. code-block:: Python


    # removing checkpoints

    for file in os.listdir(output_paths["checkpoints"]):
        # remove file
        os.remove(os.path.join(output_paths["checkpoints"], file))


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.796 seconds)


.. _sphx_glr_download_auto_examples_customising_behaviour_plot_modify_layer_sizes.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_modify_layer_sizes.ipynb <plot_modify_layer_sizes.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_modify_layer_sizes.py <plot_modify_layer_sizes.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
