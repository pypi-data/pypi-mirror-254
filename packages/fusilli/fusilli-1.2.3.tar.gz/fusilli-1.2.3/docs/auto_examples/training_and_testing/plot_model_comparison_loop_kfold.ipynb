{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training multiple models in a loop: k-fold regression\n\nWelcome to the \"Comparing Multiple K-Fold Trained Fusion Models\" tutorial! In this tutorial, we'll explore how to train and compare multiple fusion models for a regression task using k-fold cross-validation with multimodal tabular data. This tutorial is designed to help you understand and implement key features, including:\n\n- \ud83d\udce5 Importing fusion models based on modality types.\n- \ud83d\udeb2 Setting training parameters for your models\n- \ud83d\udd2e Generating simulated data for experimentation.\n- \ud83e\uddea Training and evaluating multiple fusion models.\n- \ud83d\udcc8 Visualising the results of individual models.\n- \ud83d\udcca Comparing the performance of different models.\n\nLet's dive into each of these steps in detail:\n\n1. **Importing Fusion Models:**\n\n   We begin by importing fusion models based on modality types. These models will be used in our regression task, and we'll explore various fusion strategies.\n\n2. **Setting the Training Parameters:**\n\n   To ensure consistent and controlled training, we define training parameters. These parameters include enabling k-fold cross-validation, specifying the prediction type (regression), and setting the batch size for training.\n\n3. **Generating Simulated Data:**\n\n   In this step, we generate synthetic data to simulate a real-world multimodal dataset. This dataset includes two tabular modalities, but it can also incorporate image data, although we won't use images in this example.\n\n4. **Training All Fusion Models:**\n\n   Now, we train all the selected fusion models using the generated data and the defined training parameters. We'll monitor the performance of each model during training and store the results for later analysis.\n\n5. **Plotting Individual Model Results:**\n\n   After training, we visualise the performance of each individual model. We create plots that show loss curves and performance metrics to help us understand how each model performed.\n\n6. **Comparing Model Performance:**\n\n   To gain insights into which fusion models perform best, we compare their performance using a violin chart. This chart provides a clear overview of how each model's performance metrics compare.\n\n7. **Saving the Results:**\n\n   Finally, we save the performance results of all the models as a structured DataFrame. This data can be further analyzed, exported to a CSV file, or used for additional experiments.\n\nNow, let's walk through each of these steps in code and detail. Let's get started! \ud83c\udf38\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\n\nfrom docs.examples import generate_sklearn_simulated_data\nfrom fusilli.data import prepare_fusion_data\nfrom fusilli.eval import RealsVsPreds, ModelComparison\nfrom fusilli.train import train_and_save_models\nfrom fusilli.utils.model_chooser import import_chosen_fusion_models\n\n# sphinx_gallery_thumbnail_number = -1\n\n# from IPython.utils import io  # for hiding the tqdm progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import fusion models \ud83d\udd0d\nHere we import the fusion models to be compared. The models are imported using the\n:func:`~fusilli.utils.model_chooser.get_models` function, which takes a dictionary of conditions\nas an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.\n\nThe function returns a dataframe of the models that match the conditions. The dataframe contains the\nmethod name, the class name, the modality type, the fusion type, the path to the model, and the path to the\nmodel's parent class. The paths are used to import the models with the :func:`importlib.import_module`.\n\nWe're importing all the fusion models that use only tabular data for this example (either uni-modal or multi-modal).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_conditions = {\n    \"modality_type\": [\"tabular1\", \"tabular2\", \"tabular_tabular\"],\n}\n\nfusion_models = import_chosen_fusion_models(model_conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set the training parameters \ud83c\udfaf\nLet's configure our training parameters.\nFor training and testing, the necessary parameters are:\n\n- Paths to the input data files.\n- Paths to the output directories.\n- ``prediction_task``: the type of prediction to be performed. This is either ``regression``, ``binary``, or ``classification``.\n\nSome optional parameters are:\n\n- ``kfold``: a boolean of whether to use k-fold cross-validation (True) or not (False). By default, this is set to False.\n- ``num_folds``: the number of folds to use. It can't be ``k=1``.\n- ``wandb_logging``: a boolean of whether to log the results using Weights and Biases (True) or not (False). Default is False.\n- ``test_size``: the proportion of the dataset to include in the test split. Default is 0.2.\n- ``batch_size``: the batch size to use for training. Default is 8.\n- ``multiclass_dimensions``: the number of classes to use for multiclass classification. Default is None unless ``prediction_task`` is ``multiclass``.\n- ``max_epochs``: the maximum number of epochs to train for. Default is 1000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Regression task (predicting a continuous variable)\nprediction_task = \"regression\"\n\n# Set the batch size\nbatch_size = 32\n\n# Enable k-fold cross-validation with k=3\nkfold = True\nnum_folds = 3\n\n# Setting output directories\noutput_paths = {\n    \"losses\": \"loss_logs/model_comparison_loop_kfold\",\n    \"checkpoints\": \"checkpoints/model_comparison_loop_kfold\",\n    \"figures\": \"figures/model_comparison_loop_kfold\",\n}\n\nos.makedirs(output_paths[\"losses\"], exist_ok=True)\nos.makedirs(output_paths[\"checkpoints\"], exist_ok=True)\nos.makedirs(output_paths[\"figures\"], exist_ok=True)\n\n# Clearing the loss logs directory (only for the example notebooks)\nfor dir in os.listdir(output_paths[\"losses\"]):\n    # remove files\n    for file in os.listdir(os.path.join(output_paths[\"losses\"], dir)):\n        os.remove(os.path.join(output_paths[\"losses\"], dir, file))\n    # remove dir\n    os.rmdir(os.path.join(output_paths[\"losses\"], dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generating simulated data \ud83d\udd2e\nTime to create some simulated data for our models to work their wonders on.\nThis function also simulated image data which we aren't using here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tabular1_path, tabular2_path = generate_sklearn_simulated_data(prediction_task,\n                                                               num_samples=500,\n                                                               num_tab1_features=10,\n                                                               num_tab2_features=20)\n\ndata_paths = {\n    \"tabular1\": tabular1_path,\n    \"tabular2\": tabular2_path,\n    \"image\": \"\",\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the all the fusion models \ud83c\udfc1\nIn this section, we train all the fusion models using the generated data and specified parameters.\nWe store the results of each model for later analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Using %%capture to hide the progress bar and plots (there are a lot of them!)\n\nall_trained_models = {}\n\nfor i, fusion_model in enumerate(fusion_models):\n    fusion_model_name = fusion_model.__name__\n    print(f\"Running model {fusion_model_name}\")\n\n    # Get data module\n    data_module = prepare_fusion_data(prediction_task=prediction_task,\n                                      fusion_model=fusion_model,\n                                      data_paths=data_paths,\n                                      output_paths=output_paths,\n                                      kfold=kfold,\n                                      num_folds=num_folds,\n                                      batch_size=batch_size)\n\n    # Train and test\n    single_model_list = train_and_save_models(\n        data_module=data_module,\n        fusion_model=fusion_model,\n        enable_checkpointing=False,  # We're not saving the trained models for this example\n        show_loss_plot=True,  # We'll show the loss plot for each model instead of saving it\n    )\n\n    # Save to all_trained_models\n    all_trained_models[fusion_model_name] = single_model_list\n\n    plt.close(\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plotting the results of the individual models\nIn this section, we visualize the results of each individual model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for model_name, model_list in all_trained_models.items():\n    fig = RealsVsPreds.from_final_val_data(model_list)\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plotting comparison of the models\nIn this section, we visualize the results of each individual model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "comparison_plot, metrics_dataframe = ModelComparison.from_final_val_data(all_trained_models)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Saving the results of the models\nIn this section, we compare the performance of all the trained models using a violin chart, providing an overview of how each model performed as a distribution over the different cross-validation folds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metrics_dataframe"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}