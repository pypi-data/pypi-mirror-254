{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Regression: Comparing Two Tabular Models Trained on Simulated Data\n\n\ud83d\ude80 Welcome to this tutorial on training and comparing two fusion models on a regression task using simulated multimodal tabular data! \ud83c\udf89\n\n\ud83c\udf1f Key Features:\n\n- \ud83d\udce5 Importing models based on name.\n- \ud83e\uddea Training and testing models with train/test protocol.\n- \ud83d\udcbe Saving trained models to a dictionary for later analysis.\n- \ud83d\udcca Plotting the results of a single model.\n- \ud83d\udcc8 Plotting the results of multiple models as a bar chart.\n- \ud83d\udcbe Saving the results of multiple models as a CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import importlib\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\n\nfrom docs.examples import generate_sklearn_simulated_data\nfrom fusilli.data import prepare_fusion_data\nfrom fusilli.eval import RealsVsPreds, ModelComparison\nfrom fusilli.train import train_and_save_models\nfrom fusilli.utils.model_chooser import import_chosen_fusion_models\n\n# sphinx_gallery_thumbnail_number = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import fusion models \ud83d\udd0d\nLet's kick things off by importing our fusion models. The models are imported using the\n:func:`~fusilli.utils.model_chooser.import_chosen_fusion_models` function, which takes a dictionary of conditions\nas an input. The conditions are the attributes of the models, e.g. the class name, the modality type, etc.\n\nThe function returns list of class objects that match the conditions. If no conditions are specified, then all the models are returned.\n\nWe're importing ConcatTabularData and TabularChannelWiseMultiAttention models for this example. Both are multimodal tabular models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_conditions = {\n    \"class_name\": [\"ConcatTabularData\", \"TabularChannelWiseMultiAttention\"],\n}\n\nfusion_models = import_chosen_fusion_models(model_conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set the training parameters \ud83c\udfaf\nNow, let's configure our training parameters. The parameters are stored in a dictionary and passed to most\nof the methods in this library.\n\nFor training and testing, the necessary parameters are:\n\n- Paths to the input data files.\n- Paths to the output directories.\n- ``prediction_task``: the type of prediction to be performed. This is either ``regression``, ``binary``, or ``classification``.\n\nSome optional parameters are:\n\n- ``kfold``: a boolean of whether to use k-fold cross-validation (True) or not (False). By default, this is set to False.\n- ``num_folds``: the number of folds to use. It can't be ``k=1``.\n- ``wandb_logging``: a boolean of whether to log the results using Weights and Biases (True) or not (False). Default is False.\n- ``test_size``: the proportion of the dataset to include in the test split. Default is 0.2.\n- ``batch_size``: the batch size to use for training. Default is 8.\n- ``multiclass_dimensions``: the number of classes to use for multiclass classification. Default is None unless ``prediction_task`` is ``multiclass``.\n- ``max_epochs``: the maximum number of epochs to train for. Default is 1000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Regression task (predicting a binary variable - 0 or 1)\nprediction_task = \"regression\"\n\n# Set the batch size\nbatch_size = 48\n\n# Set the test_size\ntest_size = 0.3\n\n# Setting output directories\noutput_paths = {\n    \"losses\": \"loss_logs/two_models_traintest\",\n    \"checkpoints\": \"checkpoints/two_models_traintest\",\n    \"figures\": \"figures/two_models_traintest\",\n}\n\nfor path in output_paths.values():\n    os.makedirs(path, exist_ok=True)\n\n# Clearing the loss logs directory (only for the example notebooks)\nfor dir in os.listdir(output_paths[\"losses\"]):\n    # remove files\n    for file in os.listdir(os.path.join(output_paths[\"losses\"], dir)):\n        os.remove(os.path.join(output_paths[\"losses\"], dir, file))\n    # remove dir\n    os.rmdir(os.path.join(output_paths[\"losses\"], dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generating simulated data \ud83d\udd2e\nTime to create some simulated data for our models to work their wonders on.\nThis function also simulated image data which we aren't using here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tabular1_path, tabular2_path = generate_sklearn_simulated_data(prediction_task,\n                                                               num_samples=500,\n                                                               num_tab1_features=10,\n                                                               num_tab2_features=20)\n\ndata_paths = {\n    \"tabular1\": tabular1_path,\n    \"tabular2\": tabular2_path,\n    \"image\": \"\",\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the first fusion model \ud83c\udfc1\nHere we train the first fusion model. We're using the ``train_and_save_models`` function to train and test the models.\nThis function takes the following inputs:\n\n- ``prediction_task``: the type of prediction to be performed.\n- ``fusion_model``: the fusion model to be trained.\n- ``data_paths``: the paths to the input data files.\n- ``output_paths``: the paths to the output directories.\n\nFirst we'll create a dictionary to store both the trained models so we can compare them later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_trained_models = {}  # create dictionary to store trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train the first model we need to:\n\n1. *Choose the model*: We're using the first model in the ``fusion_models`` list we made earlier.\n2. *Print the attributes of the model*: To check it's been initialised correctly.\n3. *Create the datamodule*: This is done with the :func:`~fusilli.data.prepare_fusion_data` function. This function takes the initialised model and some parameters as inputs. It returns the datamodule.\n4. *Train and test the model*: This is done with the :func:`~fusilli.train.train_and_save_models` function. This function takes the datamodule and the fusion model as inputs, as well as optional training modifications. It returns the trained model.\n5. *Add the trained model to the ``all_trained_models`` dictionary*: This is so we can compare the results of the two models later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fusion_model = fusion_models[0]\n\nprint(\"Method name:\", fusion_model.method_name)\nprint(\"Modality type:\", fusion_model.modality_type)\nprint(\"Fusion type:\", fusion_model.fusion_type)\n\n# Create the data module\ndm = prepare_fusion_data(prediction_task=prediction_task,\n                         fusion_model=fusion_model,\n                         data_paths=data_paths,\n                         output_paths=output_paths,\n                         batch_size=batch_size,\n                         test_size=test_size)\n\n# train and test\nmodel_1_list = train_and_save_models(\n    data_module=dm,\n    fusion_model=fusion_model,\n    enable_checkpointing=False,  # False for the example notebooks\n    show_loss_plot=True,\n)\n\n# Add trained model to dictionary\nall_trained_models[fusion_model.__name__] = model_1_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plotting the results of the first model \ud83d\udcca\nLet's unveil the results of our first model's hard work. We're using the :class:`~fusilli.eval.RealsVsPreds` class to plot the results of the first model.\nThis class takes the trained model as an input and returns a plot of the real values vs the predicted values from the final validation data (when using from_final_val_data).\nIf you want to plot the results from the test data, you can use from_new_data instead. See the example notebook on plotting with new data for more detail.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reals_preds_model_1 = RealsVsPreds.from_final_val_data(model_1_list)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training the second fusion model \ud83c\udfc1\n It's time for our second fusion model to shine! Here we train the second fusion model: TabularChannelWiseMultiAttention. We're using the same steps as before, but this time we're using the second model in the ``fusion_models`` list.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fusion_model = fusion_models[1]\n\nprint(\"Method name:\", fusion_model.method_name)\nprint(\"Modality type:\", fusion_model.modality_type)\nprint(\"Fusion type:\", fusion_model.fusion_type)\n\n# Create the data module\ndm = prepare_fusion_data(prediction_task=prediction_task,\n                         fusion_model=fusion_model,\n                         data_paths=data_paths,\n                         output_paths=output_paths,\n                         batch_size=batch_size,\n                         test_size=test_size)\n\n# train and test\nmodel_2_list = train_and_save_models(\n    data_module=dm,\n    fusion_model=fusion_model,\n    enable_checkpointing=False,  # False for the example notebooks\n    show_loss_plot=True,\n)\n\n# Add trained model to dictionary\nall_trained_models[fusion_model.__name__] = model_2_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Plotting the results of the second model \ud83d\udcca\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reals_preds_model_2 = RealsVsPreds.from_final_val_data(model_2_list)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparing the results of the two models \ud83d\udcc8\nLet the ultimate showdown begin! We're comparing the results of our two models.\nWe're using the :class:`~fusilli.eval.ModelComparison` class to compare the results of the two models.\nThis class takes the trained models as an input and returns a plot of the results of the two models and a Pandas DataFrame of the metrics of the two models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "comparison_plot, metrics_dataframe = ModelComparison.from_final_val_data(\n    all_trained_models\n)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Saving the metrics of the two models \ud83d\udcbe\nTime to archive our models' achievements. We're using the :class:`~fusilli.eval.ModelComparison` class to save the metrics of the two models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metrics_dataframe"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}