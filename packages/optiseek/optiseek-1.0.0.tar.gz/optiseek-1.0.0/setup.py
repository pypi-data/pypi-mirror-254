# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['optiseek']

package_data = \
{'': ['*']}

install_requires = \
['numpy>=1.22.4,<2.0.0', 'pandas>=2.1.4,<3.0.0']

setup_kwargs = {
    'name': 'optiseek',
    'version': '1.0.0',
    'description': 'A collection of single objective optimization algorithms for multi-dimensional functions.',
    'long_description': "![optiseek logo](docs/images/optiseek_logo_small.png)\n\n[![Downloads](https://static.pepy.tech/personalized-badge/optiseek?period=total&units=none&left_color=black&right_color=red&left_text=Downloads)](https://pepy.tech/project/optiseek)\n\nAn open source collection of single-objective optimization algorithms for multi-dimensional functions.\n\nThe purpose of this library is to give users access to a variety of versatile black-box optimization algorithms with extreme ease of use and interoperability.\nThe parameters of each of the algorithms can be tuned by the users and there is a high level of parameter uniformity between algorithms.\n\nBenefits of using Optiseek include:\n\n- support for float, integer, categorical, and boolean inputs for objective functions\n- compatibility with black-box objective functions (requires no information on gradients or form of function)\n- the algorithms are simple compared to alternatives (e.g. Bayesian optimization) with faster runtime on basic objective functions\n- competitive convergence for computionally expensive objective functions in terms of number of function evaluations\n- seamless integration into ML pipelines for hyper-parameter tuning\n- access to a variety of stopping criteria, suitable for both computationally expensive and cheap objective functions\n- carefully chosen default parameters for algorithms, with ability for user-defined fine tuning\n\n## Documentation\n\nFor full documentation, visit the [github pages site](https://acdundore.github.io/optiseek/).\n\n## Installation\n\n```bash\npip install optiseek\n```\n\n## Usage\n\n### Basic Implementation\n\n`optiseek` provides access to numerous optimization algorithms that require minimal effort from the user. An example using the well-known particle swarm optimization algorithm can be as simple as this:\n\n```python\nfrom optiseek.variables import var_float\nfrom optiseek.metaheuristics import particle_swarm_optimizer\nfrom optiseek.testfunctions import booth\n\n# define a list of variables and their domains for the objective function\nvar_list = [\n\tvar_float('x1', [-10, 10]),\n\tvar_float('x2', [-10, 10])\n]\t\n\n# create an instance of the algorithm to optimize the booth test function and set its parameters\nalg = particle_swarm_optimizer(booth, var_list)\n\n# define stopping criteria and optimize\nalg.optimize(find_minimum=True, max_iter=10)\n\n# show the results!\nprint(f'best_value = {alg.best_value:.5f}')\nprint(f'best_position = {alg.best_position}')\nprint(f'n_iter = {alg.completed_iter}')\n```\n\n```profile\nbest_value = 0.00217\nbest_position = {'x1': 0.9921537320723116, 'x2': 3.0265668104168326}\nn_iter = 10\n```\n\nThis is a fairly basic example implementation without much thought put into parameter selection. Of course, the user is free to tune the parameters of the algorithm any way they would like.\n\n### Tuning ML Hyperparameters\n\nThe steps to tuning hyperparameters for an ML algorithm with `optiseek` are straighforward:\n\n1. Prepare the training data\n2. Create a function that performs cross-validation with the hyperparameters are arguments and error metric as output\n3. Define the search space\n4. Pass this to an algorithm in `optiseek` and optimize\n\n```python\n# imports\nfrom optiseek.variables import *\nfrom optiseek.metaheuristics import particle_swarm_optimizer\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import cross_validate\n\nfrom lightgbm import LGBMRegressor\n\n# import and prepare the data\ncalifornia_housing = fetch_california_housing(as_frame=True)\nX = california_housing.data\ny = california_housing.target\n\n\n# set up cross-validation of the model as the objective function\ndef objective_function(learning_rate, num_leaves, max_depth, min_child_weight, min_child_samples, subsample, colsample_bytree, reg_alpha):\n    # assign the parameters\n    params = {\n        'n_estimators': 300,\n        'learning_rate': learning_rate,\n        'num_leaves': num_leaves,\n        'max_depth': max_depth,\n        'min_child_weight': min_child_weight,\n        'min_child_samples': min_child_samples,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'reg_alpha': reg_alpha,\n        'verbose': -1\n    }\n\n    # create the model\n    model = LGBMRegressor(**params)\n\n    # cross validate and return average validation MRSE\n    cv_results = cross_validate(model, X, y, scoring='neg_root_mean_squared_error', cv=5)\n    cv_score = -np.mean(cv_results['test_score'])\n\n    return cv_score\n\n\n# define the search space\nvar_list = [\n    var_float('learning_rate', [0.001, 0.3]),\n    var_int('num_leaves', [20, 3000]),\n    var_int('max_depth', [3, 12]),\n    var_float('min_child_weight', [0.0005, 0.1]),\n    var_int('min_child_samples', [5, 50]),\n    var_float('subsample', [0.5, 1]),\n    var_float('colsample_bytree', [0.5, 1]),\n    var_float('reg_alpha', [0.001, 0.1])\n]\n\n# instantiate an optimization algorithm with the function and search domain\nalg = particle_swarm_optimizer(objective_function, var_list, results_filename='cv_results.csv')\n\n# set stopping criteria and optimize\nalg.optimize(find_minimum=True, max_function_evals=300)\n\n# show the results!\nprint(f'best_value = {alg.best_value:.5f}')\nprint(f'best_position = {alg.best_position}')\n```\n\n```profile\nbest_value = 0.60881\nbest_position = {'learning_rate': 0.24843279626513076, 'num_leaves': 3000, 'max_depth': 3, 'min_child_weight': 0.06303795879741575, 'min_child_samples': 27, 'subsample': 0.5, 'colsample_bytree': 0.9620615099733333, 'reg_alpha': 0.022922559999999998}\n```\n\n## License\n\n`optiseek` was created by Alex Dundore. It is licensed under the terms of the MIT license.\n\n## Credits and Dependencies\n\n`optiseek` is powered by [`numpy`](https://numpy.org/).",
    'author': 'Alex Dundore',
    'author_email': 'acdundore.5@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://pypi.org/project/optiseek',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
